{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee772a-e7df-4b87-912e-85e349dd42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import faker\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OrderGeneration\").getOrCreate()\n",
    "\n",
    "cities = [\"Moscow\", \"Saint Petersburg\", \"Novosibirsk\", \"Yekaterinburg\", \"Kazan\", \"Nizhny Novgorod\", \"Chelyabinsk\", \"Omsk\", \"Samara\", \"Rostov-on-Don\"]\n",
    "restaurants = [\"Café Moscow\", \"Pizza & Coffee\", \"Tea House\", \"Sushi World\", \"Burger King\", \"Cafe 24\", \"Fresh Food\", \"Pasta Bar\", \"The Diner\", \"Taste of Italy\"]\n",
    "menu_items = [(\"Pizza\", 150), (\"Tea\", 100), (\"Apple Juice\", 200), (\"Coffee\", 250), (\"Cake\", 400), (\"Soup\", 300), (\"Salad\", 250), (\"Burger\", 350)]\n",
    "cities_coords = {\n",
    "    \"Moscow\": (55.7558, 37.6173),\n",
    "    \"Saint Petersburg\": (59.9343, 30.3351),\n",
    "    \"Novosibirsk\": (55.0084, 82.9357),\n",
    "    \"Yekaterinburg\": (56.8389, 60.6057),\n",
    "    \"Kazan\": (55.8304, 49.0661),\n",
    "    \"Nizhny Novgorod\": (56.2965, 43.9361),\n",
    "    \"Chelyabinsk\": (55.1644, 61.4368),\n",
    "    \"Omsk\": (54.9924, 73.3686),\n",
    "    \"Samara\": (53.2007, 50.1500),\n",
    "    \"Rostov-on-Don\": (47.2357, 39.7015),\n",
    "}\n",
    "\n",
    "fake = faker.Faker()\n",
    "def generate_order(order_id, city, restaurant, date_time):\n",
    "    order = []\n",
    "    menu_item_count = random.randint(1, 5)\n",
    "    for _ in range(menu_item_count):\n",
    "        item, price = random.choice(menu_items)\n",
    "        order.append((order_id, city, \"Russia\", restaurant, cities_coords[city][0], cities_coords[city][1], item, price, date_time))\n",
    "    return order\n",
    "\n",
    "orders = []\n",
    "order_id = 1000000\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "date_range = (end_date - start_date).days\n",
    "\n",
    "for city in cities:\n",
    "    for restaurant in restaurants:\n",
    "        for _ in range(1000): \n",
    "            date_time = start_date + timedelta(days=random.randint(0, date_range), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n",
    "            order_data = generate_order(order_id, city, restaurant, date_time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            orders.extend(order_data)\n",
    "            order_id += 1\n",
    "\n",
    "for i in range(len(orders) // 10):\n",
    "    idx = random.randint(0, len(orders) - 1)\n",
    "    if random.random() < 0.5:\n",
    "        orders[idx] = orders[idx][:4] + (None, None, None, None) + orders[idx][8:]\n",
    "    else:\n",
    "        orders[idx] = orders[idx][:4] + (-999.999, -999.999) + orders[idx][6:]\n",
    "\n",
    "columns = [\"Order_id\", \"city\", \"country\", \"facility_name\", \"lat\", \"lng\", \"menu_item\", \"price\", \"datetime\"]\n",
    "df = spark.createDataFrame(orders, columns)\n",
    "df.write.option(\"header\", \"true\").csv(\"output/orders_data\", mode=\"overwrite\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a7eb6-d20d-4e6d-88f9-46b72f0424d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ReadAndFilterOrders\").getOrCreate()\n",
    "\n",
    "#spark = SparkSession.builder \\\n",
    " #   .appName(\"AvroExample\") \\\n",
    "  #  .config(\"spark.jars\", \"file:///home/lidakarpovich/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/pyspark/jars/spark-avro_2.12-3.1.2.jar\") \\\n",
    "   # .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AvroExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"SET spark.sql.avro.compression.codec=snappy\")\n",
    "\n",
    "input_path = \"output/orders_data\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(input_path, inferSchema=True)\n",
    "df.show(5)\n",
    "invalid_df = df.filter(\n",
    "    (col(\"lat\").isNull()) | (col(\"lng\").isNull()) | (col(\"lat\") == -999.999) | (col(\"lng\") == -999.999)\n",
    ")\n",
    "\n",
    "valid_df = df.subtract(invalid_df)\n",
    "invalid_df.write.option(\"header\", \"true\").csv(\"path/to/deadletter\", mode=\"overwrite\")\n",
    "valid_df.show(5)\n",
    "\n",
    "# Запись в Avro с партиционированием по дате и городу\n",
    "#valid_df.write.format(\"avro\").partitionBy(\"datetime\", \"city\").mode(\"overwrite\").save(\"path/to/output_avro/file_name.avro\")\n",
    "valid_df.write.format(\"avro\").partitionBy(\"datetime\", \"city\").mode(\"overwrite\").save(\"path/to/output_avro/\" + \"file_name.avro\")\n",
    "\n",
    "# Запись в Parquet с партиционированием по дате и городу\n",
    "valid_df.write.format(\"parquet\").partitionBy(\"datetime\", \"city\").mode(\"overwrite\").save(\"path/to/output_parquet\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ad805-1097-4977-95fa-39b8d24bb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AvroToPostgres\").getOrCreate()\n",
    "input_avro = \"orders_avro\"\n",
    "df = spark.read.format(\"avro\").load(input_avro)\n",
    "\n",
    "schema = df \\\n",
    "    .withColumn(\"Order_id\", col(\"Order_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"lat\", col(\"lat\").cast(DoubleType())) \\\n",
    "    .withColumn(\"lng\", col(\"lng\").cast(DoubleType())) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(IntegerType())) \\\n",
    "    .withColumn(\"datetime\", col(\"datetime\").cast(TimestampType()))\n",
    "\n",
    "# Разделение на 3НФ таблицы\n",
    "cities_df = df.select(\"city\", \"country\").distinct()\n",
    "facilities_df = df.select(\"facility_name\", \"city\", \"lat\", \"lng\").distinct()\n",
    "menu_df = df.select(\"menu_item\", \"price\").distinct()\n",
    "orders_df = df.select(\"Order_id\", \"datetime\", \"facility_name\", \"menu_item\")\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/orders_db\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "cities_df.write.jdbc(postgres_url, \"cities\", mode=\"overwrite\", properties=postgres_properties)\n",
    "facilities_df.write.jdbc(postgres_url, \"facilities\", mode=\"overwrite\", properties=postgres_properties)\n",
    "menu_df.write.jdbc(postgres_url, \"menu\", mode=\"overwrite\", properties=postgres_properties)\n",
    "orders_df.write.jdbc(postgres_url, \"orders\", mode=\"overwrite\", properties=postgres_properties)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da167fd2-adf2-4c20-82dc-33f892b678a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 4\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AvroToPostgresStar\").getOrCreate()\n",
    "\n",
    "input_avro = \"orders_avro\"\n",
    "df = spark.read.format(\"avro\").load(input_avro)\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(\"Order_id\", col(\"Order_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"lat\", col(\"lat\").cast(DoubleType())) \\\n",
    "    .withColumn(\"lng\", col(\"lng\").cast(DoubleType())) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(IntegerType())) \\\n",
    "    .withColumn(\"datetime\", col(\"datetime\").cast(TimestampType()))\n",
    "\n",
    "fact_orders_df = df.select(\n",
    "    \"Order_id\", \"datetime\", \"city\", \"country\", \"facility_name\", \"lat\", \"lng\", \"menu_item\", \"price\"\n",
    ")\n",
    "\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/orders_db\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "fact_orders_df.write.jdbc(postgres_url, \"fact_orders\", mode=\"overwrite\", properties=postgres_properties)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dcb0c-9b3b-4412-b553-4b039d40b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, struct, sum as spark_sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ParquetToMongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/orders_db.orders\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "input_parquet = \"orders_parquet\"\n",
    "df = spark.read.parquet(input_parquet)\n",
    "\n",
    "orders_aggregated_df = df.groupBy(\"Order_id\", \"city\", \"country\", \"facility_name\", \"datetime\") \\\n",
    "    .agg(\n",
    "        collect_list(struct(\"menu_item\", \"price\")).alias(\"order_items\"),\n",
    "        spark_sum(\"price\").alias(\"total_price\")\n",
    "    )\n",
    "\n",
    "orders_aggregated_df.write \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
